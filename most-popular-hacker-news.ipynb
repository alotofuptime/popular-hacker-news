{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most-popular-hacker-news🗞\n",
    "\n",
    "> This will be a web-scraping project that will scrape **https://news.ycombinator.com/** for the most popular articles based on the amount of up-votes received.\n",
    "\n",
    "## DISCLAIMER📢:\n",
    "> **This project is for `educational purposes ONLY` & does not encourage malpractice of web scraping whatsoever. `ALL` scraping done in this project will `NOT` exceed the limitations listed in `https://news.ycombinator.com/robots.txt` (shown below).**\n",
    "\n",
    "![img](https://i.imgur.com/ohRZTDr.png)\n",
    "\n",
    "> **NOTE: `ALWAYS` check https://{website of interest}.com/robots.txt `BEFORE` you attempt to scrape anything from the site. Whatever is listed as `disallowed` is not allowed to be scraped and it is against the law to do so.**\n",
    "\n",
    "### Disallowed Pages & Crawl Delay Limit Explained🤔:\n",
    "> In the screen-shot of `https://news.ycombinator.com/robots.txt` (shown above) it lists the pages we cannot scrape in the form of `relative links`. An example of a relative link would be `/robots.txt`. This relative link is added to end of `https://news.ycombinator.com` to create an `absolute link` to the `robots.txt` page of the site. To sum it up, the following pages are not allowed to be scraped:\n",
    "\n",
    "1. https://news.ycombinator.com/x?\n",
    "2. https://news.ycombinator.com/r?\n",
    "3. https://news.ycombinator.com/vote?\n",
    "4. https://news.ycombinator.com/reply?\n",
    "5. https://news.ycombinator.com/submitted?\n",
    "6. https://news.ycombinator.com/submitlink?\n",
    "7. https://news.ycombinator.com/threads?\n",
    "\n",
    "> `Crawl-Delay: 30` simply means that once we access the site we have to wait 30 seconds before we can access it again. This is something set in place to avoid the inconvenience of the website crashing.\n",
    "\n",
    "### Objective📋\n",
    "\n",
    "> Scrape hacker news for the most popular news articles of the day based on votes & sort the list of articles by most up-voted to least up-voted. \n",
    "\n",
    "### Expansion & Functionality🧩\n",
    "\n",
    "#### Keyword Feature⚙\n",
    "> Provide keyword functionality that will scrape based on whether or not any keyword in a list of given `keywords exist in the titles`. This will be something to keep in mind while building out our scraper so that we leave the door open for scalability and expansion.\n",
    "\n",
    "#### Pagination Feature⚙\n",
    "> Provide page number functionality that will allow a user to enter the `number of pages` they want to be included. This is also something that will be kept in mind throughout the building process. It would be a nice feature to add,\n",
    "\n",
    "#### Minimum Number of Votes Feature⚙\n",
    "> Provide functionality that allows a user to input a `minimum number of votes` that the articles must have. E.g., an input of 200 votes would likely filter out less popular articles completely & shorten the overall list of articles to chose from. I think this is a great feature to have. \n",
    "\n",
    "### Scraping Method🤖\n",
    "\n",
    "> I favor the asynchronous method of scraping due to it being extremely fast. (We'll cover what this means later on).\n",
    "\n",
    "### Libraries We Will Be Using📚\n",
    "\n",
    "1. `asyncio` - this library allows us to use asynchronous functions in python\n",
    "2. `aiohttp` - this is what you can call an asynchronous version of the requests library\n",
    "3. `lxml` - Beautiful Soup is a nice library but it is not xpath compatible & **xpath is the best path** when web scraping. It allows for high level precision & it is very robust. The `lxml` is an xml parsing library that also has an html parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m[jovian] Error: Failed to read the Jupyter notebook. Please re-run this cell to try again. If the issue persists, provide the \"filename\" argument to \"jovian.commit\" e.g. \"jovian.commit(filename='my-notebook.ipynb')\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"most-popular-hacker-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of Attack\n",
    "\n",
    "Now that we know what our objective is & what tools we will be using, we can begin to state exactly what relevant data we want to scrape.\n",
    "\n",
    "### Data Points of Interest\n",
    "\n",
    "#### `Article Data Points`\n",
    "1. **The news article title**\n",
    "2. **The news article link**\n",
    "3. **The number of up-votes the news article has received**\n",
    "4. **The author of the article**\n",
    "5. **The time article was published**\n",
    "\n",
    "#### `Navigation Data Points`\n",
    "1. **The `more` (next page) link** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m[jovian] Error: Failed to read the Jupyter notebook. Please re-run this cell to try again. If the issue persists, provide the \"filename\" argument to \"jovian.commit\" e.g. \"jovian.commit(filename='my-notebook.ipynb')\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surveying The Field 🔍\n",
    "> Before we start to write code, it is essential to inspect the website we're scraping to determine where exactly our desired data points reside within the html. You can do this in a few quick & easy steps.\n",
    "\n",
    "1. **Hover over the target data point with your mouse. e.g., the `article title`** \n",
    "2. **Right click** \n",
    "3. **Select inspect from the drop down menu.**\n",
    "\n",
    "![img](https://i.imgur.com/wfJIzLQ.png)\n",
    "\n",
    "### Using Developer Tools ⚒\n",
    "> Once you click inspect a small window will open up either on the right side or the bottom of the page showing the `html code of the element`. In our case it will show the html for the title of the article that we were hovering over with our mouse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.imgur.com/tbyVhOb.png)\n",
    "\n",
    "> This gives us the ability to see where exactly we will be able to find the data we are looking for. The title of the article is located within an `anchor tag` with the `class titlelink`. The link to the article is also located within the same anchor tag's `href attribute`. That's two desired data points within one tag. Perfect!\n",
    "\n",
    "#### NOTE: \n",
    "> You can also see `a.titlelink` above the element on the left side of the screen. In CSS styling the `.` is used to express a class name. This signifies the highlighted element as being an `anchor tag` with the `class titlelink` as mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XPATH is the BEST Path\n",
    "> There are two ways to locate the desired data in an html document.\n",
    "1. **CSS Selectors**\n",
    "2. **Xpath Selectors**\n",
    "\n",
    "> In my humble opinion, Xpath is the better choice because it allows us to be precise & it is a necessary skill to have when it comes to web scraping. CSS Selectors are also good to learn as well & are less complex than Xpath. The difference in complexity between the two comes at a cost. You can use whichever method that you prefer. In this project we will be using Xpath.\n",
    "\n",
    "#### Here are some great resources to learn both methods\n",
    "\n",
    "**Xpath Selectors:**\n",
    "\n",
    "1. A great video that will make you really sound with xpath created by `Automation Zone` can be found here: https://www.youtube.com/watch?v=NhG__BL8zFo. \n",
    "2. An awesome cheat sheet provide by `Dev Hints` can be found here: https://devhints.io/xpath.\n",
    "\n",
    "**CSS Selectors:**\n",
    "\n",
    "1. A video created by `Web Dev Simplified` can be found here: https://www.youtube.com/watch?v=l1mER1bV0N0\n",
    "2. `W3Schools` as a great reference that can be found here: https://www.w3schools.com/cssref/css_selectors.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating Article Data Points\n",
    "\n",
    "> If you press `CTR-F (CMD-F on Mac)` within the developer tools window, a blank text box will appear under the html code. Inside this text box we can type in an xpath (or css selector) & make sure that it is not only correct but also determine how many elements with the same xpath exist on the page.\n",
    "\n",
    "![img](https://i.imgur.com/Q7QxxDB.png)\n",
    "\n",
    "> We can see that the xpath `//a[@class=\"titlelink\"]` gives us all the `anchor tags` with the `class titlelink` on the page. On the far right of the text field we see that it says `1 of 30`. That means there are 30 elements with this xpath of this page. We can click on the `up & down arrows` to look at each one. Keep in mind that this is where two of our article data points reside.\n",
    "\n",
    "1. **The title of the article**\n",
    "2. **The link to the article**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Title\n",
    "> The title of the article is going to be the text of within the anchor tag. We can extract the text of the anchor tag by adding `/text()` to the end of the xpath like so `//a[@class=\"titlelink\"]/text()`.\n",
    "\n",
    "![img](https://i.imgur.com/9wingSl.png)\n",
    "\n",
    "**Take note of the fact that the count on the far right is still showing 30 elements. With the above xpath we will be extracting text for all 30. This will be recognized by Python as a list of strings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Link\n",
    "> The link to the article is located within the anchor tag's href attribute. To extract the href attribute we can simply add `/@href` to the end of our xpath like so `//a[@class=\"titlelink\"]/@href`.\n",
    "\n",
    "![img](https://i.imgur.com/9wingSl.png)\n",
    "\n",
    "**Take note of the fact that the count on the far is still showing 30 elements. With the above xpath we will be extracting the link for all 30. This will also be recognized by Python as a list of strings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Up-Votes\n",
    "> Using the same steps mentioned in the `Using Developer Tools` we can see that `the number of votes` is located in within a `span` element with the `class score`. We extract the text from this element the same way we extracted the text from the title of the article like so `//span[@class=\"score\"]/text()`.\n",
    "\n",
    "![img](https://i.imgur.com/3u6CzcR.png)\n",
    "\n",
    "> There are two problems with this data point. Can you pinpoint them? These two problems will provide an answer to the question you may or may not have in your head. **When are we finally going to code?!** This is a prime example of why you must take the time to plan out your project before you rush to start writing code.\n",
    "\n",
    "#### Houston, we have TWO problems! \n",
    "> 1. **There are `30 articles` on the page BUT `only 29 of them have been up-voted`! The 8th article in the screen-shot above has been posted only an hour ago and has 0 up-votes (points).**\n",
    "\n",
    "> 2. **The up-votes are represented by `points` causing the number of votes to be a `string containing the word \"points\" at the end`.**\n",
    "\n",
    "**We will have to handle these two problems within our code later on & we'll be able to do that easily. The important thing is that we know what to expect when it comes to grabbing the up-votes data.** Let's going ahead and state what we will do to handle this.\n",
    "\n",
    "### Handling Up-Vote Data \n",
    "\n",
    "#### Edge Case #1: The article has no up-votes\n",
    "> In the event that the article has no up-votes we will provide a `default value` that will be set to `0`. \n",
    "\n",
    "#### Edge Case #2: The article has up-votes & \" points\" needs to be removed\n",
    "> In the event that the article has up-votes we will `remove \" points\"` from the string in order to be able to `convert the number of up-votes(points) into an integer`. This will allow us to `sort the articles by number of votes`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Author\n",
    "> The author is located within an `anchor tag` with the `class hnuser`. We will extract the text of this element to get the name of the author with the following xpath expression: `'//a[@class=\"hnuser\"]/text()'`\n",
    "\n",
    "![img](https://i.imgur.com/LLzTN5E.png)\n",
    "\n",
    "> There are `30 articles` and `only 29 of them have the author data point`! We will have to handle this in our code. Hopefully you are starting to see the value in planning out your project before you start coding.\n",
    "\n",
    "### Handling Article Author Data \n",
    "**Edge Case #1: The article doesn't have an author\n",
    "\n",
    "> In the event that an article doesn't have the author data point we will provide a `default value` set to `'N\\A'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Publication Time\n",
    "> The published time data point is located in a `span element` with the `class age`. In this case the actual date & time information is within the `title attribute`. There is also text that simply states how many hours ago the article was published. We want the date & time information. We can extract this data point with the following xpath expression: `'//span[@class=\"age\"]/@title'` \n",
    "\n",
    "![img](https://i.imgur.com/0xbhuIp.png)\n",
    "\n",
    "> We can see that this data point is present in all 30 of the articles on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating Navigation Data Points\n",
    "> At the very bottom of the page there is a `more` link that takes you to the next page of articles. Let's inspect this element & see how we can go about `navigating the site` with our scraper.\n",
    "\n",
    "![img](https://i.imgur.com/dtVLcuW.png)\n",
    "\n",
    "> We can see that the more link is located in an `anchor tag` with the `class morelink`. We can extract the `href attribue` from the `anchor tag` to get the link using the following xpath expression: `'//a[@class=\"morelink\"]/@href'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.imgur.com/dtVLcuW.png)\n",
    "\n",
    "> There is only one element that matches this xpath expression and rightfully so since there is only one more link on the page. If you look very closely at the actual `href attribute` value you will see that is a `relative link` that reads: **\"news?p=2\"**. The `p=2` means that the link leads us to the `page number` that is equal to `2`. The fact that it is a relative link means that we can simply add it to the main url to form the absolute link in the following manner: `\"https://news.ycombinator.com/news?p=2\"`. What do you think will happen if we simply change the 2 to a 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://i.imgur.com/igxerG2.png)\n",
    "\n",
    "> Changing the 2 to a 1 at brings us to the first page! This will be very useful in our pagination feature later on. We will be able to simply change the number at the end of the link to go to different pages. For now we will state how to handle this data point below.\n",
    "\n",
    "### Handling Navigation🧭\n",
    "\n",
    "#### Edge Case #1\n",
    "> In the event that the next page is unreachable will not attempt to scrape it. The way we will handle this will be explained in the implementation phase later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"alotofuptime/most-popular-hacker-news\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/alotofuptime/most-popular-hacker-news\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/alotofuptime/most-popular-hacker-news'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's Time To Code🧑‍💻!\n",
    "\n",
    "> Now that we have a detailed plan we can start to import the necessary libraries to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asyncio --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Show HN: Test your shape rotation skills',\n",
       " 'Google Tag Manager, the new anti-adblock weapon',\n",
       " 'Be anonymous',\n",
       " 'The fastest GIF does not exist',\n",
       " 'x86 Is an Octal Machine (1992)',\n",
       " 'About adding a static route to my DOCSIS modem']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store hackernews main page in a variable url\n",
    "url =  \"https://news.ycombinator.com\"\n",
    "\n",
    "# make request to hackernews using a ClientSession context manager\n",
    "async def get_html(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            if response.status == 200:\n",
    "                html = await response.text()\n",
    "                return html\n",
    "\n",
    "# pass html response to html parser using the etree class from lxml\n",
    "parser = etree.HTMLParser()\n",
    "tree = etree.fromstring(await get_html(url), parser)\n",
    "\n",
    "# test out our article titles xpath\n",
    "article_titles = tree.xpath('//a[@class=\"titlelink\"]/text()')\n",
    "\n",
    "article_titles[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the nummber of titles. it should be 30\n",
    "len(article_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So far so good. This is just the beginning. Later we will implement each part of the process into functions to make our code cleaner. Before we do that we will do a few sanity checks to test our assumptions/logic and explore our data points. Let's check the rest of our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"alotofuptime/most-popular-hacker-news\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/alotofuptime/most-popular-hacker-news\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/alotofuptime/most-popular-hacker-news'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://0xf00ff00f.github.io/rotator/',\n",
       " 'https://chromium.woolyss.com/f/HTML-Google-Tag-Manager-the-new-anti-adblock-weapon.html',\n",
       " 'https://kg.dev/thoughts/be-anonymous',\n",
       " 'https://www.biphelps.com/blog/The-Fastest-GIF-Does-Not-Exist',\n",
       " 'https://gist.github.com/seanjensengrey/f971c20d05d4d0efc0781f2f3c0353da',\n",
       " 'https://blog.danman.eu/about-adding-a-static-route-to-my-docsis-modem/']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out our news article links xpath\n",
    "article_links = tree.xpath('//a[@class=\"titlelink\"]/@href')\n",
    "\n",
    "article_links[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the length of the article links list is 30\n",
    "len(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Great! Our first two data points are working as expected. We have realistic expectations according to the detailed plan that we laid out in advance. This helps us to focus more on coding and spend less time fixing unexpected bugs. Let's move along to the next data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['390 points',\n",
       " '12 points',\n",
       " '315 points',\n",
       " '512 points',\n",
       " '83 points',\n",
       " '37 points']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out our votes xpath\n",
    "num_of_votes = tree.xpath('//span[@class=\"score\"]/text()')\n",
    "\n",
    "num_of_votes[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the length (expected to be 29)\n",
    "len(num_of_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Imagine if you didn't plan before hand. This would be unexpected behavior and would cause you to have to do further digging in the middle of coding. Our planning has paid off! We will handle this in the near future when we start to structure our program into functions. Next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0xf00ff00f', 'thyrox', 'kashnote', 'todsacerdoti', 'a1a106ed5', 'azalemeth']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out our authors xpath\n",
    "article_authors = tree.xpath('//a[@class=\"hnuser\"]/text()')\n",
    "\n",
    "article_authors[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the lenght of authors list (expected to be 29)\n",
    "len(article_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, due to our extensive planning we already expect this result and already have determined how we will handle it. Let's continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"alotofuptime/most-popular-hacker-news\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/alotofuptime/most-popular-hacker-news\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/alotofuptime/most-popular-hacker-news'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-02-20T19:19:09',\n",
       " '2022-02-21T01:41:03',\n",
       " '2022-02-20T18:54:36',\n",
       " '2022-02-20T14:11:14',\n",
       " '2022-02-20T20:41:48',\n",
       " '2022-02-20T23:15:07']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out our published times xpath\n",
    "published_times = tree.xpath('//span[@class=\"age\"]/@title')\n",
    "\n",
    "published_times[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the length of published times(expected to be 30)\n",
    "len(published_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nice! All five of our main data points our going according to plan as for as the xpaths are concerned. Last but certainly not least, let's checkout the pagination data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news?p=2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out the more link xpath(we'll call it next_page)\n",
    "next_page = tree.xpath('//a[@class=\"morelink\"]/@href')[0]\n",
    "\n",
    "next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pagination Functionality⚙\n",
    "> Since all we have to do is change the number from 2 to the desired page number, we will make things easier on ourselves by using `string formatting` to `generate the next page url`. This will be the first feature that we create! \n",
    "\n",
    "- Name: `page_generator`\n",
    "- Type: `generator`\n",
    "- Task: `yield next page url one at a time within a given range starting from 1`\n",
    "\n",
    "##### Input\n",
    "**main_url**: `str` representing the `main page of the website`\n",
    "\n",
    "**page_count**: `int` representing the `num of pages to generate`\n",
    "\n",
    "##### Output\n",
    "**next_page**: `str` representing the `next_page in the specified page_count starting from 1`\n",
    "\n",
    "(e.g, if url is \"https://news.ycombinator.com/\" & page_count is 5 the return value will be \"https://news.ycombinator.com/news?p=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_generator(url, page_count):\n",
    "    for n in range(1, page_count + 1):\n",
    "        yield f\"{url}/news?p={n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the sake of readability, let's take our page_generator, pass in the arguments, url (defined earlier) & 5 (page_count). Then let's store it in a variable call news_feed. This makes our code more readable. With generators you have to call the built in `next()` function provided by Python to get `the next yielded result`. Since we have to call next(), it would be easier on the eyes to say `next(news_feed)` as opposed to next(next_page) or next(page_generator(url, 5))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_feed = page_generator(url, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.ycombinator.com/news?p=1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(news_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What do you think will happen if we call `next(news_feed)` again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Changes to be committed:\r\n",
      "  (use \"git restore --staged <file>...\" to unstage)\r\n",
      "\t\u001b[32mnew file:   most-popular-hacker-news.ipynb\u001b[m\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\r\n",
      "\t\u001b[31mmodified:   most-popular-hacker-news.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\t\u001b[31m.jovianrc\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add most-popular-hacker-news.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Changes to be committed:\r\n",
      "  (use \"git restore --staged <file>...\" to unstage)\r\n",
      "\t\u001b[32mnew file:   most-popular-hacker-news.ipynb\u001b[m\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31m.ipynb_checkpoints/\u001b[m\r\n",
      "\t\u001b[31m.jovianrc\u001b[m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "*** Please tell me who you are.\r\n",
      "\r\n",
      "Run\r\n",
      "\r\n",
      "  git config --global user.email \"you@example.com\"\r\n",
      "  git config --global user.name \"Your Name\"\r\n",
      "\r\n",
      "to set your account's default identity.\r\n",
      "Omit --global to set the identity only in this repository.\r\n",
      "\r\n",
      "fatal: unable to auto-detect email address (got 'jovyan@jupyter-alotofuptime--api-2dgit-2de223aaa-2d6ae1987c8bfc-5f53-2.(none)')\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"added page generator feauter📃\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"jmarcano617@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
